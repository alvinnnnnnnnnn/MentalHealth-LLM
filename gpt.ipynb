{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import psycopg2\n",
    "import evaluate\n",
    "import time \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, pipeline, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results\"\n",
    "LOG_DIR = \"logs\"\n",
    "TRAIN_FILE = \"dataset/train.txt\"  # Path to your training data file\n",
    "VAL_FILE = \"dataset/validation.txt\"  # Path to your validation data file\n",
    "TEST_FILE = \"dataset/test.txt\"  # Path to your test data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size: 49152\n"
     ]
    }
   ],
   "source": [
    "original_vocab_size = model.config.vocab_size\n",
    "print(f\"Original vocab size: {original_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max length: 8192\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model max length: {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 8192\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(49152, 960, padding_idx=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (23489 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tokens: 23489\n",
      "Validation Tokens: 6160\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(file_path, tokenizer):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Count tokens in training and validation files\n",
    "train_token_count = count_tokens(TRAIN_FILE, tokenizer)\n",
    "eval_token_count = count_tokens(VAL_FILE, tokenizer)\n",
    "print(f\"Train Tokens: {train_token_count}\")\n",
    "print(f\"Validation Tokens: {eval_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",     # Logs loss at intervals\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=2,  # Reduced batch size for limited GPU memory\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=LOG_DIR,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch sizes\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    optim=\"adamw_torch\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chatbot_data(file_path):\n",
    "    \"\"\"Load and preprocess chatbot data from the given text file.\"\"\"\n",
    "    conversations = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        user_input, bot_response = None, None\n",
    "        for line in lines:\n",
    "            if line.startswith(\"user:\"):\n",
    "                user_input = line.replace(\"user:\", \"\").strip()\n",
    "            elif line.startswith(\"bot:\"):\n",
    "                bot_response = line.replace(\"bot:\", \"\").strip()\n",
    "                if user_input and bot_response:\n",
    "                    conversations.append({\"input\": user_input, \"output\": bot_response})\n",
    "                    user_input, bot_response = None, None\n",
    "    return pd.DataFrame(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 278\n",
      "Length of validation dataset: 70\n"
     ]
    }
   ],
   "source": [
    "df_train = load_chatbot_data(TRAIN_FILE)\n",
    "df_val = load_chatbot_data(VAL_FILE)\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "print(f\"Length of training dataset: {len(df_train)}\")\n",
    "print(f\"Length of validation dataset: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max token length in dataset: 146\n",
      "Token length distribution (first 10 samples): [99, 97, 72, 82, 48, 75, 67, 70, 63, 57]\n"
     ]
    }
   ],
   "source": [
    "def find_max_interaction_length(dataset):\n",
    "    length_distribution = []\n",
    "\n",
    "    for user_msg, bot_msg in zip(dataset[\"input\"], dataset[\"output\"]):\n",
    "        # Combine user message and bot response\n",
    "        text_pair = f\"User: {user_msg} Bot: {bot_msg}\"\n",
    "\n",
    "        # Tokenize\n",
    "        tokenized = tokenizer(text_pair, truncation=False, padding=False)\n",
    "\n",
    "        # Get the token length of this interaction\n",
    "        num_tokens = len(tokenized[\"input_ids\"])\n",
    "        length_distribution.append(num_tokens)\n",
    "\n",
    "    # Find the maximum token length\n",
    "    max_length = max(length_distribution)\n",
    "\n",
    "    return max_length, length_distribution\n",
    "\n",
    "dataset = load_chatbot_data(\"dataset/dataset.txt\")\n",
    "max_len, token_lengths = find_max_interaction_length(dataset)\n",
    "\n",
    "print(f\"Max token length in dataset: {max_len}\")\n",
    "print(f\"Token length distribution (first 10 samples): {token_lengths[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(examples):\n",
    "#     inputs = [f\"{inp} {out}\" for inp, out in zip(examples[\"input\"], examples[\"output\"])]\n",
    "#     model_inputs = tokenizer(\n",
    "#         inputs,\n",
    "#         max_length=1024,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         return_tensors=\"pt\",\n",
    "#     )\n",
    "#     model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
    "\n",
    "#     num_tokens = len(model_inputs[\"input_ids\"][0])\n",
    "#     print(f\"Number of tokens: {num_tokens}\")\n",
    "#     return model_inputs\n",
    "\n",
    "# # Apply tokenization\n",
    "# tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
    "# tokenized_val = dataset_val.map(tokenize_function, batched=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cd1790c6fc4ad89b6a2186a29be50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/278 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fa892843cf4c8fb446e17111418f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/70 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "\n",
    "    for user_msg, bot_msg in zip(examples[\"input\"], examples[\"output\"]):\n",
    "        text_pair = f\"User: {user_msg} Bot: {bot_msg}\"\n",
    "        tokenized = tokenizer(\n",
    "            text_pair,\n",
    "            max_length=200,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Store tokenized outputs\n",
    "        model_inputs[\"input_ids\"].append(tokenized[\"input_ids\"].squeeze(0))\n",
    "        model_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"].squeeze(0))\n",
    "\n",
    "        # Labels for training: Shift left for causal language modeling\n",
    "        labels = tokenized[\"input_ids\"].clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n",
    "        model_inputs[\"labels\"].append(labels.squeeze(0))\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
    "tokenized_val = dataset_val.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training samples: 278\n",
      "Total validation samples: 70\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total training samples: {len(tokenized_train)}\")\n",
    "print(f\"Total validation samples: {len(tokenized_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Masked language modeling is not used for causal LM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wongq\\AppData\\Local\\Temp\\ipykernel_23520\\2601145598.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 36:00, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.842600</td>\n",
       "      <td>1.652153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.534500</td>\n",
       "      <td>1.591750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.417500</td>\n",
       "      <td>1.569976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.342900</td>\n",
       "      <td>1.562569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.269600</td>\n",
       "      <td>1.560466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=170, training_loss=1.4814085118910845, metrics={'train_runtime': 2163.666, 'train_samples_per_second': 0.642, 'train_steps_per_second': 0.079, 'total_flos': 519525642240000.0, 'train_loss': 1.4814085118910845, 'epoch': 4.9784172661870505})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer.processing_class = tokenizer.__class__\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results\\\\tokenizer_config.json',\n",
       " 'results\\\\special_tokens_map.json',\n",
       " 'results\\\\vocab.json',\n",
       " 'results\\\\merges.txt',\n",
       " 'results\\\\added_tokens.json',\n",
       " 'results\\\\tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = TEST_FILE\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    chat_data = file.readlines()\n",
    "\n",
    "# Extract user inputs and reference responses\n",
    "user_inputs = []\n",
    "reference_responses = []\n",
    "\n",
    "for i in range(len(chat_data) - 1):\n",
    "    if chat_data[i].startswith(\"user:\"):\n",
    "        user_text = chat_data[i].replace(\"user:\", \"\").strip()\n",
    "        ref_text = chat_data[i + 1].replace(\"bot:\", \"\").strip() if chat_data[i + 1].startswith(\"bot:\") else None\n",
    "        \n",
    "        if ref_text:  # Ensure reference answer exists\n",
    "            user_inputs.append(user_text)\n",
    "            reference_responses.append(ref_text)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame({\"input\": user_inputs, \"reference_response\": reference_responses})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Custom Model, GPT2 and Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = AutoModelForCausalLM.from_pretrained(\"results\", device_map=\"auto\")\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained(\"results\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", use_auth_token=True)\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate reponses for all 3 models based on each prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Define models and their corresponding tokenizers in a list\n",
    "models = [\n",
    "    {\"name\": \"llama_model\", \"model\": llama_model.to(device), \"tokenizer\": llama_tokenizer},\n",
    "    {\"name\": \"gpt_model\", \"model\": gpt_model.to(device), \"tokenizer\": gpt_tokenizer},\n",
    "    {\"name\": \"custom_model\", \"model\": custom_model.to(device), \"tokenizer\": custom_tokenizer}\n",
    "]\n",
    "\n",
    "prompt_column = \"input\"  # Update if your column name is different\n",
    "\n",
    "# Function to generate chatbot response using the corresponding tokenizer\n",
    "def chatbot_response(prompt, model, tokenizer):\n",
    "    system_prompt = \"You are a helpful and supportive chatbot. Answer the user's question with empathy, and in a clear and concise way without repeating their words exactly.\"\n",
    "    full_prompt = f\"{system_prompt}\\nUser: {prompt}\\nBot:\"\n",
    "\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=350,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3,  \n",
    "        temperature=0.3,  \n",
    "        top_p=0.9,  \n",
    "        top_k=50  \n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Clean up response\n",
    "    response = response.replace(system_prompt, \"\").strip()\n",
    "    if \"Bot:\" in response:\n",
    "        response = response.split(\"Bot:\")[-1].strip()\n",
    "\n",
    "    return response\n",
    "\n",
    "for model_info in models:\n",
    "    model_name = model_info[\"name\"]\n",
    "    model = model_info[\"model\"]\n",
    "    tokenizer = model_info[\"tokenizer\"]\n",
    "    \n",
    "    df[f\"{model_name}_response\"] = df[prompt_column].apply(lambda prompt: chatbot_response(prompt, model, tokenizer))\n",
    "\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chatbot_response(prompt):\n",
    "#     system_prompt = \"You are a helpful and supportive chatbot. Answer the user's question with empathy, and in a clear and concise way without repeating their words exactly.\"\n",
    "#     full_prompt = f\"{system_prompt}\\nUser: {prompt}\\nBot:\"\n",
    "\n",
    "#     inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "#     inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "#     temperature=0.8\n",
    "#     outputs = custom_model.generate(\n",
    "#         **inputs, \n",
    "#         max_length=300,\n",
    "#         repetition_penalty=1.3,\n",
    "#         no_repeat_ngram_size=3,  \n",
    "#         temperature=temperature,  \n",
    "#         top_p=0.9,  #\n",
    "#         top_k=50  \n",
    "#     )\n",
    "\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#     # Ensure the response does not include the system prompt\n",
    "#     response = response.replace(system_prompt, \"\").strip()\n",
    "    \n",
    "#     # Remove any leftover system prompt instructions\n",
    "#     if \"Bot:\" in response:\n",
    "#         response = response.split(\"Bot:\")[-1].strip()\n",
    "\n",
    "#     return response\n",
    "\n",
    "# df[\"model_response\"] = df[\"input\"].apply(chatbot_response)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # **🔹 STEP 4: Load BERTScore Metric**\n",
    "# bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# def compute_bertscore(predictions, references):\n",
    "#     results = bertscore.compute(predictions=predictions, references=references, model_type=\"microsoft/deberta-xlarge-mnli\")\n",
    "#     return results[\"f1\"]  # Extract F1 BERTScore\n",
    "\n",
    "# # Compute BERTScores\n",
    "# df[\"BERTScore_Model\"] = compute_bertscore(df[\"model_response\"].tolist(), df[\"input\"].tolist())  # Model's response vs Input\n",
    "# df[\"BERTScore_Reference\"] = compute_bertscore(df[\"reference_response\"].tolist(), df[\"input\"].tolist())  # Reference vs Input\n",
    "\n",
    "# # **🔹 STEP 6: Compute BERTScore Difference**\n",
    "# df[\"BERTScore_Difference\"] = abs(df[\"BERTScore_Model\"] - df[\"BERTScore_Reference\"])  # Absolute Deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate BERTScore for each model reponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "# Initialize BERTScorer\n",
    "scorer = BERTScorer(model_type=\"bert-base-uncased\")\n",
    "\n",
    "# List of model response columns\n",
    "model_response_columns = [\"llama_model_response\", \"gpt_model_response\", \"custom_model_response\"]\n",
    "\n",
    "for model_col in model_response_columns:\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    f1_scores = []\n",
    "    time_taken = []  # Store time taken for each response\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        reference = row[\"reference_response\"]\n",
    "        candidate = row[model_col]\n",
    "\n",
    "        # Record start time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute BERTScore\n",
    "        P, R, F1 = scorer.score([candidate], [reference])\n",
    "\n",
    "        # Record end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Compute and store time taken\n",
    "        time_elapsed = end_time - start_time\n",
    "        time_taken.append(time_elapsed)\n",
    "\n",
    "        # Store results\n",
    "        precision_scores.append(P.item())  # Convert tensor to float\n",
    "        recall_scores.append(R.item())\n",
    "        f1_scores.append(F1.item())\n",
    "\n",
    "    # Store scores in the DataFrame\n",
    "    df[f\"{model_col}_BERT_Precision\"] = precision_scores\n",
    "    df[f\"{model_col}_BERT_Recall\"] = recall_scores\n",
    "    df[f\"{model_col}_BERT_F1\"] = f1_scores  # F1 is the main score to look at\n",
    "    df[f\"{model_col}_Time_Taken\"] = time_taken  # Store time taken for each response\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Empathy Score for each model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "c:\\Users\\wongq\\anaconda3\\envs\\python3.9\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# **🔹 STEP 7: Load Empathy Model (RoBERTa-based classifier)**\n",
    "empathy_model_name = \"unitary/unbiased-toxic-roberta\"\n",
    "empathy_tokenizer = AutoTokenizer.from_pretrained(empathy_model_name)\n",
    "empathy_model = AutoModelForSequenceClassification.from_pretrained(empathy_model_name)\n",
    "empathy_pipeline = pipeline(\"text-classification\", model=empathy_model, tokenizer=empathy_tokenizer, return_all_scores=True)\n",
    "\n",
    "# **🔹 STEP 8: Compute Empathy Score**\n",
    "def compute_empathy(text):\n",
    "    scores = empathy_pipeline(text)[0]  # Get model confidence scores\n",
    "    empathetic_score = scores[1]['score'] if len(scores) > 1 else 0  # Index 1 corresponds to \"non-toxic/empathy\"\n",
    "    return empathetic_score\n",
    "\n",
    "df[\"Empathy_Score\"] = df[\"model_response\"].apply(compute_empathy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_bertscore_model = df[\"BERTScore_Model\"].mean()\n",
    "avg_bertscore_reference = df[\"BERTScore_Reference\"].mean()\n",
    "avg_bertscore_difference = df[\"BERTScore_Difference\"].mean()\n",
    "avg_empathy_score = df[\"Empathy_Score\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BERTScore Model: 0.554422102502135\n",
      "Average BERTScore Reference: 0.6004213055626291\n",
      "Average BERTScore Difference: 0.04904351322377314\n",
      "Average Empathy Score: 2.3390437680988174e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average BERTScore Model: {avg_bertscore_model}\")\n",
    "print(f\"Average BERTScore Reference: {avg_bertscore_reference}\")\n",
    "print(f\"Average BERTScore Difference: {avg_bertscore_difference}\")\n",
    "print(f\"Average Empathy Score: {avg_empathy_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mace_tools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtools\u001b[39;00m\n\u001b[0;32m      2\u001b[0m tools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBERTScore Deviation + Empathy Analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mdf)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import ace_tools as tools\n",
    "tools.display_dataframe_to_user(name=\"BERTScore Deviation + Empathy Analysis\", dataframe=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "USER = os.getenv(\"user\")\n",
    "PASSWORD = os.getenv(\"password\")\n",
    "HOST = os.getenv(\"host\")\n",
    "PORT = os.getenv(\"port\")\n",
    "DBNAME = os.getenv(\"dbname\")\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        user=USER,\n",
    "        password=PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT,\n",
    "        dbname=DBNAME\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "    \n",
    "    # Create a cursor to execute SQL queries\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def store_conversation(user_message, bot_response, sentiment):\n",
    "    \"\"\"Stores a conversation in Supabase PostgreSQL.\"\"\"\n",
    "    embedding = embedding_model.encode([user_message])[0].tolist()\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO conversations (user_message, bot_response, sentiment, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (user_message, bot_response, sentiment, embedding)\n",
    "        )\n",
    "        connection.commit()\n",
    "        print(\"Stored conversation\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_past_conversations(query):\n",
    "    query_embedding = embedding_model.encode([query])[0] \n",
    "\n",
    "    if isinstance(query_embedding, np.ndarray): \n",
    "        query_embedding = query_embedding.tolist()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT timestamp, user_message, bot_response FROM conversations \"\n",
    "        \"ORDER BY embedding <-> %s::vector LIMIT 5\",\n",
    "        (query_embedding,)  # Ensure it's passed as a tuple\n",
    "    )\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    if results:\n",
    "        context = \"\\n\".join([f\"[{r[0]}] User: {r[1]}\\nBot: {r[2]}\" for r in results])\n",
    "        return context\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"results\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"results\", device_map=\"auto\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for translation and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text2text-generation\", model=\"Varine/opus-mt-zh-en-model\")\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"Improved sentiment analysis with keyword-based distress detection.\"\"\"\n",
    "    crisis_keywords = [\"end my life\", \"suicide\", \"don't want to live\", \"kill myself\", \"worthless\", \"no reason to live\"]\n",
    "\n",
    "    # Check if crisis words are in the input\n",
    "    if any(phrase in text.lower() for phrase in crisis_keywords):\n",
    "        return \"crisis\"  # Override sentiment if crisis words are detected\n",
    "\n",
    "    # Otherwise, use DistilBERT-based sentiment analysis\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    label = result['label']\n",
    "\n",
    "    # Convert to sentiment categories based on DistilBERT outputs\n",
    "    if label == \"NEGATIVE\":\n",
    "        return \"negative\"\n",
    "    elif label == \"POSITIVE\":\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(prompt):\n",
    "    retrieved_context = retrieve_past_conversations(prompt)\n",
    "    translated_prompt = pipe(prompt)[0].get(\"generated_text\", \"Translation failed\")  \n",
    "\n",
    "    print(translated_prompt)\n",
    "\n",
    "    system_prompt = \"You are a helpful and supportive chatbot. Answer the user's question in a clear and concise way without repeating their words exactly.\"\n",
    "    full_prompt = f\"{system_prompt}\\n{retrieved_context}\\nUser: {prompt}\\nBot:\"\n",
    "\n",
    "    sentiment_results = get_sentiment(prompt)\n",
    "\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=650,\n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3,  \n",
    "        temperature=0.8,  \n",
    "        top_p=0.9,  #\n",
    "        top_k=50  \n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the response does not include the system prompt\n",
    "    response = response.replace(system_prompt, \"\").strip()\n",
    "    \n",
    "    # Remove any leftover system prompt instructions\n",
    "    if \"Bot:\" in response:\n",
    "        response = response.split(\"Bot:\")[-1].strip()\n",
    "\n",
    "    # Translate response to Chinese\n",
    "    # translated = pipe(response)[0]['translation_text']\n",
    "    translated_text = pipe(response)[0].get(\"generated_text\", \"Translation failed\")  # Use .get() to avoid KeyError\n",
    "\n",
    "    return response, translated_text, sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatbot is ready! Type 'exit' to stop.\")\n",
    "    while True:        \n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"DB connection ended\")\n",
    "            break\n",
    "        response, translated, sentiment_results = chatbot_response(user_input)\n",
    "        store_conversation(user_input, response, sentiment_results)\n",
    "        print(f\"User: {user_input}\")\n",
    "        print(f\"Bot: {response}\")\n",
    "        print(f\"Translated Text: {translated}\")\n",
    "        print(f\"Sentiment Results: {sentiment_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
