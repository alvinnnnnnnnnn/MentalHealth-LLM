{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import warnings\n",
    "import evaluate\n",
    "import platform\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bert_score import score\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, pipeline\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results\"\n",
    "LOG_DIR = \"logs\"\n",
    "TRAIN_FILE = \"dataset/train.txt\"  # Path to your training data file\n",
    "VAL_FILE = \"dataset/val.txt\"  # Path to your validation data file\n",
    "TEST_FILE = \"dataset/test.txt\"  # Path to your test data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e9d05f323d45a4bdaebf4b765203ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3020f8d6bb4f6e9678bc4a2ec271f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a537cde7371e41729581e5b12dc7675f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca7290a4c7f47a89a16e4840081f096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66d7e7b6ca54af6aa5636952a342b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf9ab7361e4488c82cb9965c47e7dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da093a99831a4c948dd47726dfc1eca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7377ea738c4b84968b5e50e232844d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(49152, 960, padding_idx=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tokens: 7144\n",
      "Validation Tokens: 1580\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(file_path, tokenizer):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Count tokens in training and validation files\n",
    "train_token_count = count_tokens(TRAIN_FILE, tokenizer)\n",
    "eval_token_count = count_tokens(VAL_FILE, tokenizer)\n",
    "print(f\"Train Tokens: {train_token_count}\")\n",
    "print(f\"Validation Tokens: {eval_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",     # Logs loss at intervals\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,  # Reduced batch size for limited GPU memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=LOG_DIR,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch sizes\n",
    "    fp16=True,\n",
    "    bf16=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chatbot_data(file_path):\n",
    "    \"\"\"Load and preprocess chatbot data from the given text file.\"\"\"\n",
    "    conversations = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        user_input, bot_response = None, None\n",
    "        for line in lines:\n",
    "            if line.startswith(\"User:\"):\n",
    "                user_input = line.replace(\"User:\", \"\").strip()\n",
    "            elif line.startswith(\"Bot:\"):\n",
    "                bot_response = line.replace(\"Bot:\", \"\").strip()\n",
    "                if user_input and bot_response:\n",
    "                    conversations.append({\"input\": user_input, \"output\": bot_response})\n",
    "                    user_input, bot_response = None, None\n",
    "    return pd.DataFrame(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 96\n",
      "Length of validation dataset: 24\n"
     ]
    }
   ],
   "source": [
    "df_train = load_chatbot_data(TRAIN_FILE)\n",
    "df_val = load_chatbot_data(VAL_FILE)\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "print(f\"Length of training dataset: {len(df_train)}\")\n",
    "print(f\"Length of validation dataset: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd57e5f15424e85bd89e73001902a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/96 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff65423b3a3542089bbbf2280325b6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    inputs = [f\"{inp} {out}\" for inp, out in zip(examples[\"input\"], examples[\"output\"])]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone() \n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
    "tokenized_val = dataset_val.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Masked language modeling is not used for causal LM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(references, candidates, model_type=\"roberta-large\"):\n",
    "    \"\"\"Compute BERTScore for chatbot responses.\"\"\"\n",
    "    P, R, F1 = score(candidates, references, model_type=model_type, lang=\"en\", rescale_with_baseline=True)\n",
    "    return {\n",
    "        \"Precision\": P.mean().item(),\n",
    "        \"Recall\": R.mean().item(),\n",
    "        \"F1 Score\": F1.mean().item()\n",
    "    }\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute accuracy, BERTScore, and ESAT for evaluation.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Decode model outputs\n",
    "    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n",
    "\n",
    "    # Token-level accuracy\n",
    "    pred_ids = np.argmax(predictions, axis=-1)\n",
    "    accuracy = accuracy_metric.compute(predictions=pred_ids, references=labels)\n",
    "\n",
    "    # Compute BERTScore (similarity of response to ground truth)\n",
    "    bert_scores = compute_bertscore(decoded_labels, decoded_preds)\n",
    "\n",
    "    # Combine metrics\n",
    "    final_metrics = {\n",
    "        \"Accuracy\": accuracy[\"accuracy\"],\n",
    "        **bert_scores,\n",
    "    }\n",
    "\n",
    "    print(\"\\nEvaluation Metrics:\", final_metrics)\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_d/vbh48gmd31s0b7r48gvh7m3w0000gn/T/ipykernel_2229/695427204.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "fp16 mixed precision requires a GPU (not 'mps').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/transformers/trainer.py:461\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_in_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_accelerator_and_postprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker \u001b[38;5;241m=\u001b[39m TrainerMemoryTracker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mskip_memory_metrics)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/transformers/trainer.py:5099\u001b[0m, in \u001b[0;36mTrainer.create_accelerator_and_postprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5096\u001b[0m     args\u001b[38;5;241m.\u001b[39mupdate(accelerator_config)\n\u001b[1;32m   5098\u001b[0m \u001b[38;5;66;03m# create accelerator object\u001b[39;00m\n\u001b[0;32m-> 5099\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator \u001b[38;5;241m=\u001b[39m \u001b[43mAccelerator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5100\u001b[0m \u001b[38;5;66;03m# some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\u001b[39;00m\n\u001b[1;32m   5101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather_for_metrics\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/accelerate/accelerator.py:540\u001b[0m, in \u001b[0;36mAccelerator.__init__\u001b[0;34m(self, device_placement, split_batches, mixed_precision, gradient_accumulation_steps, cpu, dataloader_config, deepspeed_plugin, fsdp_plugin, torch_tp_plugin, megatron_lm_plugin, rng_types, log_with, project_dir, project_config, gradient_accumulation_plugin, step_scheduler_with_optimizer, kwargs_handlers, dynamo_backend, dynamo_plugin, deepspeed_plugins)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnative_amp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmusa\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_torch_xla_available(\n\u001b[1;32m    538\u001b[0m     check_is_tpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    539\u001b[0m ):\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp16 mixed precision requires a GPU (not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    541\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler\u001b[38;5;241m.\u001b[39mto_kwargs() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;241m=\u001b[39m get_grad_scaler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: fp16 mixed precision requires a GPU (not 'mps')."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results\\\\tokenizer_config.json',\n",
       " 'results\\\\special_tokens_map.json',\n",
       " 'results\\\\vocab.json',\n",
       " 'results\\\\merges.txt',\n",
       " 'results\\\\added_tokens.json',\n",
       " 'results\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"results\")\n",
    "tokenizer.save_pretrained(\"results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"results\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"results\", device_map=\"auto\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chatbot_response(user_input, chatbot):\n",
    "#     sentiment = get_sentiment(user_input)  # Get sentiment from BERT\n",
    "\n",
    "#     # Define chatbot responses based on sentiment\n",
    "#     responses = {\n",
    "#         \"positive\": \"I'm happy to hear that! Keep up the great energy. 😊\",\n",
    "#         \"neutral\": \"That sounds interesting. Tell me more, I'm here to listen. 🧡\",\n",
    "#         \"negative\": \"I'm sorry you're feeling this way. You're not alone—would you like some self-care tips? 💙\"\n",
    "#     }\n",
    "\n",
    "#     # Modify chatbot input based on sentiment\n",
    "#     modified_input = f\"{user_input} [Sentiment: {sentiment.capitalize()}]\"\n",
    "\n",
    "#     # Get chatbot response\n",
    "#     bot_reply = chatbot.generate_response(modified_input)  # Assuming chatbot has a generate_response method\n",
    "\n",
    "#     # Modify response based on sentiment if needed\n",
    "#     if sentiment == \"negative\":\n",
    "#         bot_reply += \" Remember, you're not alone. Would you like some coping tips? 💙\"\n",
    "\n",
    "#     return bot_reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongq\\anaconda3\\envs\\python3.9\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"Improved sentiment analysis with keyword-based distress detection.\"\"\"\n",
    "    crisis_keywords = [\"end my life\", \"suicide\", \"don't want to live\", \"kill myself\", \"worthless\", \"no reason to live\"]\n",
    "\n",
    "    # Check if crisis words are in the input\n",
    "    if any(phrase in text.lower() for phrase in crisis_keywords):\n",
    "        return \"crisis\"  # Override sentiment if crisis words are detected\n",
    "\n",
    "    # Otherwise, use DistilBERT-based sentiment analysis\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    label = result['label']\n",
    "\n",
    "    # Convert to sentiment categories based on DistilBERT outputs\n",
    "    if label == \"NEGATIVE\":\n",
    "        return \"negative\"\n",
    "    elif label == \"POSITIVE\":\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chatbot_response(prompt, max_length=200):\n",
    "#     system_prompt = \"You are a helpful and supportive chatbot. Answer the user's question in a clear and concise way without repeating their words exactly.\"\n",
    "#     full_prompt = f\"{system_prompt}\\nUser: {prompt}\\nBot:\"\n",
    "\n",
    "#     sentiment_results = get_sentiment(prompt)\n",
    "\n",
    "#     inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "#     inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "#     outputs = model.generate(**inputs, max_length=max_length)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#     # Ensure the response does not include the system prompt\n",
    "#     response = response.replace(system_prompt, \"\").strip()\n",
    "    \n",
    "#     # Remove any leftover system prompt instructions\n",
    "#     if \"Bot:\" in response:\n",
    "#         response = response.split(\"Bot:\")[-1].strip()\n",
    "\n",
    "#     # Translate response to Chinese\n",
    "#     translated = pipe(response)[0]['translation_text']\n",
    "\n",
    "#     return response, translated, sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(prompt, max_length=200):\n",
    "    system_prompt = \"You are a helpful and supportive chatbot. Answer the user's question in a clear and concise way without repeating their words exactly.\"\n",
    "    full_prompt = f\"{system_prompt}\\nUser: {prompt}\\nBot:\"\n",
    "\n",
    "    sentiment_results = get_sentiment(prompt)\n",
    "\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        max_length=max_length,\n",
    "        repetition_penalty=1.3,  # Penalize repeated words/phrases\n",
    "        no_repeat_ngram_size=3,  # Prevent 3-word repetition\n",
    "        temperature=0.7,  # Lower randomness to keep responses meaningful\n",
    "        top_p=0.9,  # Diverse sampling to avoid looping responses\n",
    "        top_k=50  # Limits token choices to avoid repetition\n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the response does not include the system prompt\n",
    "    response = response.replace(system_prompt, \"\").strip()\n",
    "    \n",
    "    # Remove any leftover system prompt instructions\n",
    "    if \"Bot:\" in response:\n",
    "        response = response.split(\"Bot:\")[-1].strip()\n",
    "\n",
    "    # Translate response to Chinese\n",
    "    translated = pipe(response)[0]['translation_text']\n",
    "\n",
    "    return response, translated, sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to stop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongq\\anaconda3\\envs\\python3.9\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\wongq\\anaconda3\\envs\\python3.9\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: i think my mental health is getting better \n",
      "Bot: I'm glad to hear that you're feeling more positive about your well-being! It takes time, but small steps can make big differences over time. What specific areas of mind wellness would like me to help with?\n",
      "Translated Text: 我很高兴听到你对自己的福祉感到更积极!这需要时间,但小步骤会随着时间而产生很大的差异。什么具体领域的心智健康需要我帮助?\n",
      "Sentiment Results: negative\n",
      "User: i do not think about harming myself anymore these days\n",
      "Bot: I'm sorry to hear that you're feeling this way, but it can be really hard when thoughts of self-harm come up unexpectedly. It might help if we talked more openly with each other or sought support from someone who understands what you need. You deserve care and happiness. If there is something specific going on for which you feel like talking, please reach out. We all go through tough times sometimes.\n",
      "Translated Text: 我很遗憾听到你这样想, 但突然出现自我伤害的想法时会很难。 如果我们彼此更公开地交谈或寻求理解你需要的东西的人的支持, 可能会有帮助。 你应该得到关心和幸福。 如果有具体的事情发生, 你愿意与之交谈, 请伸出援手。 我们有时会经历艰难的时刻 。\n",
      "Sentiment Results: positive\n",
      "User: i am happy\n",
      "Bot: I'm glad to hear that! You're feeling good about yourself today, which is wonderful. How can I help you?\n",
      "Translated Text: 很高兴听到你这么说!\n",
      "Sentiment Results: positive\n",
      "User: i have turned to cutting myself to relieve pressure from school\n",
      "Bot: I'm really sorry you're feeling this way, it can be hard when things feel overwhelming at times. Cutting yourself is not an effective coping mechanism for stress or anxiety - please reach out to someone who cares about your well-being. You deserve support. If there’s help available through counseling services online that might interest you, let me know!\n",
      "Translated Text: 我真的很抱歉你这样感觉, 当事情有时感到压倒的时候, 感觉会很难。 割伤自己并不是一种有效的应对压力或焦虑的机制, 请联系关心你福祉的人。 你应该得到支持。 如果通过在线咨询服务可以帮助你, 请告诉我!\n",
      "Sentiment Results: negative\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatbot is ready! Type 'exit' to stop.\")\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        response, translated, sentiment_results = chatbot_response(user_input)\n",
    "        print(f\"User: {user_input}\")\n",
    "        print(f\"Bot: {response}\")\n",
    "        print(f\"Translated Text: {translated}\")\n",
    "        print(f\"Sentiment Results: {sentiment_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
