{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "\n",
    "# Enable gradient checkpointing (optional for large models)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Resize token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Load and preprocess the data\n",
    "def load_chatbot_data(file_path):\n",
    "    \"\"\"Load and preprocess chatbot data from the given text file.\"\"\"\n",
    "    conversations = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        user_input, bot_response = None, None\n",
    "        for line in lines:\n",
    "            if line.startswith(\"User:\"):\n",
    "                user_input = line.replace(\"User:\", \"\").strip()\n",
    "            elif line.startswith(\"Bot:\"):\n",
    "                bot_response = line.replace(\"Bot:\", \"\").strip()\n",
    "                if user_input and bot_response:\n",
    "                    conversations.append({\"input\": user_input, \"output\": bot_response})\n",
    "                    user_input, bot_response = None, None\n",
    "    return pd.DataFrame(conversations)\n",
    "\n",
    "# Prepare datasets\n",
    "TRAIN_FILE = \"dataset/train.txt\"\n",
    "VAL_FILE = \"dataset/val.txt\"\n",
    "df_train = load_chatbot_data(TRAIN_FILE)\n",
    "df_val = load_chatbot_data(VAL_FILE)\n",
    "\n",
    "print(df_train.isnull().sum())\n",
    "print(df_val.isnull().sum())\n",
    "\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"input\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(examples[\"output\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    model_inputs = {key: torch.tensor(val).to(device) for key, val in model_inputs.items()}  \n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
    "tokenized_val = dataset_val.map(tokenize_function, batched=True)\n",
    "\n",
    "# ✅ Use the correct data collator for a causal model\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Important: Disable MLM for autoregressive models\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"logs\",\n",
    "    max_grad_norm=1.0, # Clips gradients to prevent instability\n",
    "    gradient_accumulation_steps=2,  # Accumulates gradients over 2 steps\n",
    "    fp16=True,  # Enable mixed precision for better GPU performance\n",
    "    bf16=False,  # Disable bf16\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # ✅ Correct collator for causal models\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"results/test1_cb\") \n",
    "tokenizer.save_pretrained(\"results/test1_cb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"results/test1_cb\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"results/test1_cb\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(prompt, max_length=100):\n",
    "    # Tokenize input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    # Generate response\n",
    "    outputs = model.generate(**inputs, max_length=max_length)\n",
    "\n",
    "    # Decode the generated text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatbot is ready! Type 'exit' to stop.\")\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        response = chatbot_response(user_input)\n",
    "        print(f\"User: {user_input}\")\n",
    "        print(f\"Bot: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
