{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/alvinwong/anaconda3/envs/python3.9/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_PQbackendPID'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3.9/lib/python3.9/site-packages/psycopg2/__init__.py:51\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"A Python driver for PostgreSQL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mpsycopg is a PostgreSQL_ database adapter for the Python_ programming\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    TimeFromTicks, Timestamp, TimestampFromTicks\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# psycopg/__init__.py - initialization of the psycopg module\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2003-2019 Federico Di Gregorio  <fog@debian.org>\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Import the DBAPI-2.0 stuff into top-level module.\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_psycopg\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (                     \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     BINARY, NUMBER, STRING, DATETIME, ROWID,\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m     Binary, Date, Time, Timestamp,\n\u001b[1;32m     55\u001b[0m     DateFromTicks, TimeFromTicks, TimestampFromTicks,\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m     Error, \u001b[38;5;167;01mWarning\u001b[39;00m, DataError, DatabaseError, ProgrammingError, IntegrityError,\n\u001b[1;32m     58\u001b[0m     InterfaceError, InternalError, NotSupportedError, OperationalError,\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m     _connect, apilevel, threadsafety, paramstyle,\n\u001b[1;32m     61\u001b[0m     __version__, __libpq_version__,\n\u001b[1;32m     62\u001b[0m )\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Register default adapters.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extensions \u001b[38;5;28;01mas\u001b[39;00m _ext\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/alvinwong/anaconda3/envs/python3.9/lib/python3.9/site-packages/psycopg2/_psycopg.cpython-39-darwin.so, 0x0002): symbol not found in flat namespace '_PQbackendPID'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bert_score import score\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"results\"\n",
    "LOG_DIR = \"logs\"\n",
    "TRAIN_FILE = \"dataset/train.txt\"  # Path to your training data file\n",
    "VAL_FILE = \"dataset/validation.txt\"  # Path to your validation data file\n",
    "TEST_FILE = \"dataset/test.txt\"  # Path to your test data file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-360M-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vocab size: 49152\n"
     ]
    }
   ],
   "source": [
    "original_vocab_size = model.config.vocab_size\n",
    "print(f\"Original vocab size: {original_vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model max length: 8192\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model max length: {model.config.max_position_embeddings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length: 8192\n"
     ]
    }
   ],
   "source": [
    "print(f\"Max sequence length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(49152, 960, padding_idx=2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tokens: 27546\n",
      "Validation Tokens: 7033\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(file_path, tokenizer):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Count tokens in training and validation files\n",
    "train_token_count = count_tokens(TRAIN_FILE, tokenizer)\n",
    "eval_token_count = count_tokens(VAL_FILE, tokenizer)\n",
    "print(f\"Train Tokens: {train_token_count}\")\n",
    "print(f\"Validation Tokens: {eval_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",     # Logs loss at intervals\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=1,  # Reduced batch size for limited GPU memory\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=LOG_DIR,\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch sizes\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    optim=\"adamw_torch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chatbot_data(file_path):\n",
    "    \"\"\"Load and preprocess chatbot data from the given text file.\"\"\"\n",
    "    conversations = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        user_input, bot_response = None, None\n",
    "        for line in lines:\n",
    "            if line.startswith(\"user:\"):\n",
    "                user_input = line.replace(\"user:\", \"\").strip()\n",
    "            elif line.startswith(\"bot:\"):\n",
    "                bot_response = line.replace(\"bot:\", \"\").strip()\n",
    "                if user_input and bot_response:\n",
    "                    conversations.append({\"input\": user_input, \"output\": bot_response})\n",
    "                    user_input, bot_response = None, None\n",
    "    return pd.DataFrame(conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training dataset: 327\n",
      "Length of validation dataset: 82\n"
     ]
    }
   ],
   "source": [
    "df_train = load_chatbot_data(TRAIN_FILE)\n",
    "df_val = load_chatbot_data(VAL_FILE)\n",
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_val = Dataset.from_pandas(df_val)\n",
    "\n",
    "print(f\"Length of training dataset: {len(df_train)}\")\n",
    "print(f\"Length of validation dataset: {len(df_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170aa2120f0e4df0a637e0ffad0235c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/327 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611d31ecc74e41268d892e6994c6e299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes user-bot conversation pairs efficiently while handling long sequences\n",
    "    by chunking messages and preventing excessive padding.\n",
    "    \"\"\"\n",
    "    model_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "\n",
    "    for user_msg, bot_msg in zip(examples[\"input\"], examples[\"output\"]):\n",
    "        # Combine each user-bot pair\n",
    "        text_pair = f\"User: {user_msg} Bot: {bot_msg}\"\n",
    "\n",
    "        # Tokenize with truncation and padding\n",
    "        tokenized = tokenizer(\n",
    "            text_pair,\n",
    "            max_length=8192,  # Change if using a different model with a smaller/larger limit\n",
    "            truncation=True,  # Truncate if longer than max_length\n",
    "            padding=\"longest\",  # Use \"longest\" instead of \"max_length\" to save space\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Store tokenized outputs\n",
    "        model_inputs[\"input_ids\"].append(tokenized[\"input_ids\"].squeeze(0))\n",
    "        model_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"].squeeze(0))\n",
    "\n",
    "        # Labels for training: Shift left for causal language modeling\n",
    "        labels = tokenized[\"input_ids\"].clone()\n",
    "        labels[labels == tokenizer.pad_token_id] = -100  # Ignore padding tokens in loss\n",
    "        model_inputs[\"labels\"].append(labels.squeeze(0))\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
    "tokenized_val = dataset_val.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Masked language modeling is not used for causal LM\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bertscore(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Decode tokenized predictions & labels into text\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute BERTScore\n",
    "    P, R, F1 = score(decoded_preds, decoded_labels, lang=\"en\", rescale_with_baseline=True)\n",
    "    \n",
    "    return {\"bert_score_f1\": F1.mean().item()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wongq\\AppData\\Local\\Temp\\ipykernel_9356\\2146127477.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='810' max='810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [810/810 1:00:37, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.697900</td>\n",
       "      <td>1.612151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.395900</td>\n",
       "      <td>1.559951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.231200</td>\n",
       "      <td>1.547998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.105000</td>\n",
       "      <td>1.556723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.988500</td>\n",
       "      <td>1.587398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.893300</td>\n",
       "      <td>1.611524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>1.644920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>1.670334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.687200</td>\n",
       "      <td>1.696878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=810, training_loss=1.032991122022087, metrics={'train_runtime': 3639.7207, 'train_samples_per_second': 0.898, 'train_steps_per_second': 0.223, 'total_flos': 501429084076800.0, 'train_loss': 1.032991122022087, 'epoch': 9.880733944954128})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_bertscore,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('results\\\\tokenizer_config.json',\n",
       " 'results\\\\special_tokens_map.json',\n",
       " 'results\\\\vocab.json',\n",
       " 'results\\\\merges.txt',\n",
       " 'results\\\\added_tokens.json',\n",
       " 'results\\\\tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dotenv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mload_dotenv\u001b[49m()\n\u001b[1;32m      3\u001b[0m USER \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m PASSWORD \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassword\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dotenv' is not defined"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "USER = os.getenv(\"user\")\n",
    "PASSWORD = os.getenv(\"password\")\n",
    "HOST = os.getenv(\"host\")\n",
    "PORT = os.getenv(\"port\")\n",
    "DBNAME = os.getenv(\"dbname\")\n",
    "\n",
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        user=USER,\n",
    "        password=PASSWORD,\n",
    "        host=HOST,\n",
    "        port=PORT,\n",
    "        dbname=DBNAME\n",
    "    )\n",
    "    print(\"Connection successful!\")\n",
    "    \n",
    "    # Create a cursor to execute SQL queries\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def store_conversation(user_message, bot_response, sentiment):\n",
    "    \"\"\"Stores a conversation in Supabase PostgreSQL.\"\"\"\n",
    "    embedding = embedding_model.encode([user_message])[0]  # Generate 1024-dim vector\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO conversations (user_message, bot_response, sentiment, embedding) VALUES (%s, %s, %s, %s)\",\n",
    "            (user_message, bot_response, sentiment, embedding)\n",
    "        )\n",
    "        connection.commit()\n",
    "        print(\"Stored conversation\")\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_past_conversations(query):\n",
    "    \"\"\"Retrieves the most relevant past conversations using vector similarity.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query])[0]  # Generates a NumPy array\n",
    "\n",
    "    # ðŸ”¹ Convert NumPy array to a Python list\n",
    "    query_embedding = query_embedding.tolist()\n",
    "\n",
    "    cursor.execute(\n",
    "        \"SELECT timestamp, user_message, bot_response FROM conversations ORDER BY embedding <-> %s LIMIT 5\",\n",
    "        (query_embedding,)  # ðŸ”¹ Ensure it's inside a tuple (trailing comma is needed)\n",
    "    )\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    if results:\n",
    "        context = \"\\n\".join([f\"[{r[0]}] User: {r[1]}\\nBot: {r[2]}\" for r in results])\n",
    "        return context\n",
    "    else:\n",
    "        return \"No past conversations found.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongq\\anaconda3\\envs\\python3.9\\lib\\site-packages\\accelerate\\utils\\modeling.py:1536: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 960, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "          (k_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (v_proj): Linear(in_features=960, out_features=320, bias=False)\n",
       "          (o_proj): Linear(in_features=960, out_features=960, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (up_proj): Linear(in_features=960, out_features=2560, bias=False)\n",
       "          (down_proj): Linear(in_features=2560, out_features=960, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((960,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=960, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"results\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"results\", device_map=\"auto\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for translation and sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wongq\\anaconda3\\envs\\python3.9\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "sentiment_classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    \"\"\"Improved sentiment analysis with keyword-based distress detection.\"\"\"\n",
    "    crisis_keywords = [\"end my life\", \"suicide\", \"don't want to live\", \"kill myself\", \"worthless\", \"no reason to live\"]\n",
    "\n",
    "    # Check if crisis words are in the input\n",
    "    if any(phrase in text.lower() for phrase in crisis_keywords):\n",
    "        return \"crisis\"  # Override sentiment if crisis words are detected\n",
    "\n",
    "    # Otherwise, use DistilBERT-based sentiment analysis\n",
    "    result = sentiment_classifier(text)[0]\n",
    "    label = result['label']\n",
    "\n",
    "    # Convert to sentiment categories based on DistilBERT outputs\n",
    "    if label == \"NEGATIVE\":\n",
    "        return \"negative\"\n",
    "    elif label == \"POSITIVE\":\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(prompt):\n",
    "    retrieved_context = retrieve_past_conversations(prompt)\n",
    "\n",
    "    system_prompt = \"You are a helpful and supportive chatbot. Answer the user's question in a clear and concise way without repeating their words exactly.\"\n",
    "    full_prompt = f\"{system_prompt}\\n{retrieved_context}\\nUser: {prompt}\\nBot:\"\n",
    "\n",
    "    sentiment_results = get_sentiment(prompt)\n",
    "\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs, \n",
    "        repetition_penalty=1.3,\n",
    "        no_repeat_ngram_size=3,  \n",
    "        temperature=0.8,  \n",
    "        top_p=0.9,  #\n",
    "        top_k=50  \n",
    "    )\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the response does not include the system prompt\n",
    "    response = response.replace(system_prompt, \"\").strip()\n",
    "    \n",
    "    # Remove any leftover system prompt instructions\n",
    "    if \"Bot:\" in response:\n",
    "        response = response.split(\"Bot:\")[-1].strip()\n",
    "\n",
    "    # Translate response to Chinese\n",
    "    translated = pipe(response)[0]['translation_text']\n",
    "\n",
    "    return response, translated, sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot is ready! Type 'exit' to stop.\n"
     ]
    },
    {
     "ename": "ProgrammingError",
     "evalue": "can't adapt type 'numpy.float32'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDB connection ended\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m response, translated, sentiment_results \u001b[38;5;241m=\u001b[39m \u001b[43mchatbot_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m store_conversation(user_input, response, sentiment_results)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m, in \u001b[0;36mchatbot_response\u001b[1;34m(prompt)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchatbot_response\u001b[39m(prompt):\n\u001b[1;32m----> 2\u001b[0m     retrieved_context \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_past_conversations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful and supportive chatbot. Answer the user\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms question in a clear and concise way without repeating their words exactly.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m     full_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msystem_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mretrieved_context\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBot:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[45], line 5\u001b[0m, in \u001b[0;36mretrieve_past_conversations\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the most relevant past conversations using vector similarity.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m query_embedding \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode([query])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT timestamp, user_message, bot_response FROM conversations ORDER BY embedding <-> \u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m LIMIT 5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m results \u001b[38;5;241m=\u001b[39m cursor\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results:\n",
      "\u001b[1;31mProgrammingError\u001b[0m: can't adapt type 'numpy.float32'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Chatbot is ready! Type 'exit' to stop.\")\n",
    "    while True:        \n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            cursor.close()\n",
    "            connection.close()\n",
    "            print(\"DB connection ended\")\n",
    "            break\n",
    "        response, translated, sentiment_results = chatbot_response(user_input)\n",
    "        store_conversation(user_input, response, sentiment_results)\n",
    "        print(f\"User: {user_input}\")\n",
    "        print(f\"Bot: {response}\")\n",
    "        print(f\"Translated Text: {translated}\")\n",
    "        print(f\"Sentiment Results: {sentiment_results}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
