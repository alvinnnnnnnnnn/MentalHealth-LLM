{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re  # For regular expressions (if needed)\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_pdf_by_sections(pdf_path, section_markers=None):  # section_markers can be regex or list of headings\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    if section_markers:  # Automatic section detection\n",
    "        if isinstance(section_markers, list): # List of headings\n",
    "            for marker in section_markers:\n",
    "                # Use regex to find start and end of section\n",
    "                matches = re.finditer(rf\"(^{marker}$)([\\s\\S]*?)(?=(^{section_markers[section_markers.index(marker) + 1]}$)|$)\", text, re.MULTILINE) if section_markers.index(marker) < len(section_markers) - 1 else re.finditer(rf\"(^{marker}$)([\\s\\S]*?)$\", text, re.MULTILINE)\n",
    "                for match in matches:\n",
    "                    chunks.append(match.group(2).strip())\n",
    "        elif isinstance(section_markers, str): # Regex for section start\n",
    "             matches = re.finditer(section_markers, text, re.MULTILINE)\n",
    "             for match in matches:\n",
    "                start = match.start()\n",
    "                end = text.find(section_markers, start + 1) if text.find(section_markers, start + 1) != -1 else len(text)\n",
    "                chunks.append(text[start:end].strip())\n",
    "\n",
    "    else:  # Manual annotation (example)\n",
    "        # (Implementation for reading section boundaries from a separate file or markers in the text)\n",
    "        # ... (Add your logic here) ...\n",
    "        pass # replace with your logic\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Example usage (automatic - list of headings):\n",
    "section_headings = [\"Introduction\", \"Symptoms\", \"Treatment\", \"Prevention\"]  # Your actual section headings\n",
    "chunks = chunk_pdf_by_sections(\"mental_health.pdf\", section_headings)\n",
    "\n",
    "# Example usage (automatic - regex):\n",
    "section_regex = r\"^##\\s*(.+)$\"  # Regex to find section starts (e.g. Markdown headings)\n",
    "chunks = chunk_pdf_by_sections(\"mental_health.pdf\", section_regex)\n",
    "\n",
    "# Example usage (manual annotation):\n",
    "# chunks = chunk_pdf_by_sections(\"mental_health.pdf\") # You'd need to implement the manual parsing\n",
    "\n",
    "with open(\"my_dataset.txt\", \"w\") as outfile:\n",
    "    for chunk in chunks:\n",
    "        outfile.write(chunk + \"\\n\")  # Each chunk on a new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your full dataset\n",
    "dataset = load_dataset(\"text\", data_files={\"full\": \"your_dataset.txt\"})\n",
    "\n",
    "# Split into train and (val+test)\n",
    "train_dataset, eval_test_dataset = train_test_split(dataset[\"full\"], test_size=0.3, random_state=42) # Adjust test_size as needed\n",
    "\n",
    "# Split (val+test) into validation and test\n",
    "eval_dataset, test_dataset = train_test_split(eval_test_dataset, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = load_dataset(\"text\", data_files={\"train\": train_dataset})\n",
    "eval_dataset = load_dataset(\"text\", data_files={\"eval\": eval_dataset})\n",
    "test_dataset = load_dataset(\"text\", data_files={\"test\": test_dataset})\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"train\"])\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"eval\"])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=[\"test\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
